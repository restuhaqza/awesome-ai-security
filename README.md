# Awesome AI Security [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

A curated list of awesome resources, tools, research papers, and projects related to AI Security. This repository aims to provide a comprehensive collection of information at the intersection of Artificial Intelligence and Security, helping researchers, practitioners, and enthusiasts stay up-to-date with the latest developments in this rapidly evolving field.

## Contents
- [Frameworks and Standards](#frameworks-and-standards)
- [Tools and Frameworks](#tools-and-frameworks)
- [Research Papers](#research-papers)
- [Conferences and Events](#conferences-and-events)
- [Blogs and Articles](#blogs-and-articles)
- [Community Resources](#community-resources)
- [Key Topics](#key-topics)
- [Contributing](#contributing)

## Frameworks and Standards
- [NIST AI Risk Management Framework](https://www.nist.gov/itl/ai-risk-management-framework) - A comprehensive framework for managing risks associated with AI, focusing on risk assessment, mitigation, and governance.
- [ISO/IEC 23894:2023](https://www.iso.org/standard/77304.html) - Guidance on risk management for artificial intelligence.
- [Google Secure AI Framework](https://cloud.google.com/blog/products/identity-security/introducing-google-cloud-secure-ai-framework) - A framework for securely implementing AI systems.
- [ISO/IEC 42001](https://www.iso.org/standard/84560.html) - Artificial Intelligence Management System (under development).

## Tools and Frameworks

### Defensive Tools
- [ProtectAI's model scanner](https://github.com/protectai/model-scanner) - A security scanner for detecting suspicious actions in serialized ML models.
- [rebuff](https://github.com/woop/rebuff) - A prompt injection detector.
- [langkit](https://github.com/whylabs/langkit) - A toolkit for monitoring language models and detecting attacks.

### Detection and Analysis
- [StringSifter](https://github.com/fireeye/stringsifter) - A tool that ranks strings based on their relevance for malware analysis.
- [PentestGPT](https://github.com/GreyDGL/PentestGPT) - An interactive pentest tool integrating with OpenAI to conduct comprehensive penetration tests.

### Evasion and Injection Research
- [Adversarial Demonstration Attacks on Large Language Models](https://arxiv.org/abs/2305.14950) - Research on evasion techniques against LLMs.
- [Black Box Adversarial Prompting for Foundation Models](https://arxiv.org/abs/2302.04237) - Methods for indirect prompt injection in multi-modal LLMs.
- [DeepPayload](https://github.com/jinghangli98/DeepPayload) - Black-box backdoor attack on deep learning models through neural payload injection.

### Model Security Tools
- [HiddenLayer Model Scanner](https://hiddenlayer.com/solutions/model-scanner/) - Scans models for vulnerabilities and supply chain issues.
- [Robust Intelligence AI Firewall](https://www.robustintelligence.com/product-aifire) - Real-time protection configured to address model-specific vulnerabilities.
- [Lakera Guard](https://www.lakera.ai/) - Protection from prompt injections, data loss, and toxic content.

## Research Papers
- [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572) - Goodfellow et al., 2014. Seminal work on adversarial examples in deep learning.
- [Crafting Adversarial Input Sequences for Recurrent Neural Networks](https://arxiv.org/abs/1604.08275) - Papernot et al., 2016. Study on adversarial attacks against RNNs.
- [Robust Physical-World Attacks on Deep Learning Models](https://arxiv.org/abs/1707.08945) - Kurakin et al., 2017. Analysis of physical-world attacks on deep learning visual classifiers.
- [Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks](https://arxiv.org/abs/1701.04143) - Behzadan & Munir, 2017. Exploration of vulnerabilities in deep reinforcement learning systems.

## Conferences and Events
- [NIST Trustworthy & Responsible AI Resource Center](https://www.nist.gov/topics/artificial-intelligence/ai-risk-management-framework) - Regularly updates on AI security events and workshops.
- [DEFCON AI Village](https://aivillage.org/) - Annual event focusing on AI security at DEFCON.
- [NeurIPS Workshop on Security in Machine Learning](https://secml-workshop.github.io/) - Academic workshop on machine learning security.

## Blogs and Articles
- [Google AI Security Blog](https://security.googleblog.com/search/label/AI%20security)
- [Microsoft AI Security and Ethical AI](https://www.microsoft.com/en-us/ai/responsible-ai)
- [Institute for Ethical AI & Machine Learning Blog](https://ethical.institute/blog.html)

## Community Resources
- [OWASP AI Security and Privacy Guide](https://owasp.org/www-project-ai-security-and-privacy-guide/)
- [ENISA Multilayer Framework for Good Cybersecurity Practices for AI](https://www.enisa.europa.eu/publications/multilayer-framework-for-good-cybersecurity-practices-for-ai)
- [MLSecOps Top 10 by Institute for Ethical AI & Machine Learning](https://ethical.institute/mlsecops.html)

## Key Topics
- AI in Cybersecurity
  - Threat Detection and Response
  - Anomaly Detection
  - Automated Security Systems
- AI-Driven Threats
  - AI-Generated Phishing
  - AI-Powered Malware and Ransomware
  - Deepfakes and Social Engineering
- Secure AI Transformation
  - Data Protection in AI Systems
  - AI Model Security
  - Encryption for AI
- Zero Trust Security Models
- AI Ethics and Regulation
- Visual Intelligence in Security
- Adversarial Machine Learning
- AI in Penetration Testing

## Contributing
Your contributions are always welcome! Please read the [contribution guidelines](CONTRIBUTING.md) first.

## License
This project is licensed under the [MIT License](LICENSE).

---

If you find this repository helpful, please consider giving it a star ⭐️ to show your support!

Last updated: 2024-10-06
